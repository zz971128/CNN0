{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from glob import glob"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# In this section, we will apply an CNN to extract features and implement a classification task.\n",
    "# Firstly, we should build the model by PyTorch. We provide a baseline model here.\n",
    "# You can use your own model for better performance\n",
    "class Doubleconv_33(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Doubleconv_33, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(ch_in, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "\n",
    "class Doubleconv_35(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Doubleconv_35, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(ch_in, ch_out, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "\n",
    "class Doubleconv_37(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Doubleconv_37, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(ch_in, ch_out, kernel_size=7),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=7),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "\n",
    "class Tripleconv(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Tripleconv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(ch_in, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(ch_out, ch_out, kernel_size=3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(ch_in, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, ch_out),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.fc(input)\n",
    "\n",
    "\n",
    "class Mscnn(nn.Module):\n",
    "    # TODO: Build a better model\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(Mscnn, self).__init__()\n",
    "        self.conv11 = Doubleconv_33(ch_in, 64)\n",
    "        self.pool11 = nn.MaxPool1d(3, stride=3)\n",
    "        self.conv12 = Doubleconv_33(64, 128)\n",
    "        self.pool12 = nn.MaxPool1d(3, stride=3)\n",
    "        self.conv13 = Tripleconv(128, 256)\n",
    "        self.pool13 = nn.MaxPool1d(2, stride=2)\n",
    "        self.conv14 = Tripleconv(256, 512)\n",
    "        self.pool14 = nn.MaxPool1d(2, stride=2)\n",
    "        self.conv15 = Tripleconv(512, 512)\n",
    "        self.pool15 = nn.MaxPool1d(2, stride=2)\n",
    "\n",
    "        self.out = MLP(512*27, ch_out)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        c11 = self.conv11(x)\n",
    "        p11 = self.pool11(c11)\n",
    "        c12 = self.conv12(p11)\n",
    "        p12 = self.pool12(c12)\n",
    "        c13 = self.conv13(p12)\n",
    "        p13 = self.pool13(c13)\n",
    "        c14 = self.conv14(p13)\n",
    "        p14 = self.pool14(c14)\n",
    "        c15 = self.conv15(p14)\n",
    "        p15 = self.pool15(c15)\n",
    "        merge = p15.view(p15.size()[0], -1) \n",
    "        output = self.out(merge)\n",
    "        output = F.sigmoid(output)\n",
    "        return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Next, we need to construct the data loader for training. \n",
    "\n",
    "# These functions may be helpful for you :)\n",
    "def data_crop(data_raw, obj_len):\n",
    "    data_len = np.size(data_raw)\n",
    "    a = random.randint(0, data_len - obj_len)\n",
    "    b = a + obj_len\n",
    "    data_cropped = np.array(data_raw[:, a:b])\n",
    "    return data_cropped\n",
    "\n",
    "\n",
    "def data_pad(data_raw, obj_len):\n",
    "    data_len = np.size(data_raw)\n",
    "    pad_len = obj_len - data_len\n",
    "    b = np.zeros((1, pad_len))\n",
    "    data_padded = np.hstack((data_raw, b))\n",
    "    return data_padded\n",
    "\n",
    "class EcgDataset(data.Dataset):\n",
    "    def __init__(self,  root, data_len, transform=None, target_transform=None):    # transform=x_transform, target_transform=y_transform\n",
    "        \"\"\"\n",
    "        root: the directory of the data \n",
    "        data_len: Unknown parameters, but I think it is helpful for you :)\n",
    "        transform: pre-process for data\n",
    "        target_transform: target_transform for label\n",
    "        \"\"\"\n",
    "        self.ecgs = []\n",
    "        self.ecgs = sorted(list(glob(os.path.join(root, '*.mat'))))\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.data_len = data_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        val_dict_path = self.ecgs[index]  \n",
    "        val_dict = scio.loadmat(val_dict_path)\n",
    "        ecg_x = val_dict['value']\n",
    "        ecg_x_len = np.size(ecg_x)\n",
    "\n",
    "        # TODO: Note that there may need some pre-process for data with different sizes\n",
    "        # Write your code here\n",
    "\n",
    "        ecg_y = val_dict['label']\n",
    "        if self.transform is not None:\n",
    "            ecg_x = self.transform(ecg_x)\n",
    "            ecg_x = ecg_x.squeeze(dim=1).type(torch.FloatTensor)\n",
    "        if self.target_transform is not None:\n",
    "            ecg_y = self.target_transform(ecg_y)\n",
    "\n",
    "            ecg_y = ecg_y.squeeze(-1).type(torch.FloatTensor)\n",
    "        return ecg_x, ecg_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ecgs)\n",
    " "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Now, we will build the pipeline for deep learning based training.\n",
    "# These functions may be useful :)\n",
    "def save_loss(fold, value):\n",
    "    path = 'loss' + str(fold) + '.txt'\n",
    "    file = open(path, mode='a+')\n",
    "    file.write(str(value)+'\\n')  \n",
    "    \n",
    "# We will use GPU if cuda is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Mscnn(1, 1).to(device)   # ch_in, ch_out\n",
    "\n",
    "# Build pre-processing transformation \n",
    "# Note this pre-processing is in PyTorch\n",
    "x_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),  \n",
    "])\n",
    "y_transforms = transforms.ToTensor()\n",
    "\n",
    "\n",
    "# TODO: fine tune hyper-parameters\n",
    "batch_size = 64\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "ecg_dataset = EcgDataset('./data/train/', 2400, \n",
    "                        transform=x_transforms, target_transform=y_transforms)\n",
    "dataloader = DataLoader(ecg_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "num_epochs = 10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Start training !\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        # Write your code here\n",
    "        dt_size = len(dataloader.dataset)\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        process = tqdm(dataloader)\n",
    "        for x, y in process:\n",
    "            step += 1\n",
    "            inputs = x.to(device)\n",
    "            labels = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.squeeze(2))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            process.set_description(\n",
    "                \"epoch: %d, train_loss:%0.8f\" % (epoch, epoch_loss / step)\n",
    "            )\n",
    "        epoch_loss /= step\n",
    "       \n",
    "        save_loss(10, epoch_loss)\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'weights10_%d.pth' % (epoch))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# After training, test and evaluate your model here\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Build test dataset\n",
    "ecg_dataset = EcgDataset('./data/test', 2400, \n",
    "                             transform=x_transforms, target_transform=y_transforms)\n",
    "dataloaders = DataLoader(ecg_dataset, batch_size=1)\n",
    "\n",
    "# Set model's mode lto eval \n",
    "model.eval()\n",
    "\n",
    "# TODO: add more metrics for evaluation?\n",
    "# Evaluate \n",
    "predict = []\n",
    "target = []\n",
    "with torch.no_grad():\n",
    "    for x, mask in dataloaders:\n",
    "        y = model(x.to(device))\n",
    "        y[y >= 0.5] = 1\n",
    "        y[y < 0.5] = 0\n",
    "        predict.append(torch.squeeze(y).cpu().numpy())\n",
    "        target.append(torch.squeeze(mask).cpu().numpy())\n",
    "acc = accuracy_score(target, predict)\n",
    "print('Accuracy: {}'.format(acc))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('pujin': conda)"
  },
  "interpreter": {
   "hash": "79495757624adbb2ec94ee769202e9d295b98634352b3931ea30316c5ea35353"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}